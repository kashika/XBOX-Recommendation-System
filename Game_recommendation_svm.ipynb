{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a program to predict the games selected by users based on the queries searched by him. The dataset we have used is from besy buy and contains data for XBOX games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "train = pd.read_csv('all/train.csv')\n",
    "test = pd.read_csv('all/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing to check the accuracy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['user','category','query','click_time','query_time']]\n",
    "y = train['sku']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>category</th>\n",
       "      <th>query</th>\n",
       "      <th>click_time</th>\n",
       "      <th>query_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001cd0d10bbc585c9ba287c963e00873d4c0bfd</td>\n",
       "      <td>abcat0701002</td>\n",
       "      <td>gears of war</td>\n",
       "      <td>2011-10-09 17:22:56.101</td>\n",
       "      <td>2011-10-09 17:21:42.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00033dbced6acd3626c4b56ff5c55b8d69911681</td>\n",
       "      <td>abcat0701002</td>\n",
       "      <td>Gears of war</td>\n",
       "      <td>2011-09-25 13:35:42.198</td>\n",
       "      <td>2011-09-25 13:35:33.234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00033dbced6acd3626c4b56ff5c55b8d69911681</td>\n",
       "      <td>abcat0701002</td>\n",
       "      <td>Gears of war</td>\n",
       "      <td>2011-09-25 13:36:08.668</td>\n",
       "      <td>2011-09-25 13:35:33.234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00033dbced6acd3626c4b56ff5c55b8d69911681</td>\n",
       "      <td>abcat0701002</td>\n",
       "      <td>Assassin creed</td>\n",
       "      <td>2011-09-25 13:37:23.709</td>\n",
       "      <td>2011-09-25 13:37:00.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0007756f015345450f7be1df33695421466b7ce4</td>\n",
       "      <td>abcat0701002</td>\n",
       "      <td>dead island</td>\n",
       "      <td>2011-09-11 15:15:34.336</td>\n",
       "      <td>2011-09-11 15:15:26.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000878e35cb70ace315dbdbef54c22477066f07e</td>\n",
       "      <td>abcat0701002</td>\n",
       "      <td>Rocksmith</td>\n",
       "      <td>2011-10-25 20:44:36.756</td>\n",
       "      <td>2011-10-25 20:44:22.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0008b7e06bc3d329b4dc052268e10b98c9121452</td>\n",
       "      <td>abcat0701002</td>\n",
       "      <td>Nba n2k</td>\n",
       "      <td>2011-10-29 20:10:06.459</td>\n",
       "      <td>2011-10-29 20:09:33.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0008f35cccf771838c635196205951403f13da50</td>\n",
       "      <td>abcat0701002</td>\n",
       "      <td>Call of duty</td>\n",
       "      <td>2011-08-12 18:05:40.887</td>\n",
       "      <td>2011-08-12 18:05:06.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000c22a204a9248e3e5458ed52a084b98ddfcf56</td>\n",
       "      <td>abcat0701002</td>\n",
       "      <td>rock band</td>\n",
       "      <td>2011-10-07 00:02:54.236</td>\n",
       "      <td>2011-10-07 00:02:44.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000f573f22d30ead7a0a3d7fce64d3eb1cb8d0be</td>\n",
       "      <td>abcat0701002</td>\n",
       "      <td>xbox</td>\n",
       "      <td>2011-09-30 08:39:51.651</td>\n",
       "      <td>2011-09-30 08:36:40.246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user      category           query  \\\n",
       "0  0001cd0d10bbc585c9ba287c963e00873d4c0bfd  abcat0701002    gears of war   \n",
       "1  00033dbced6acd3626c4b56ff5c55b8d69911681  abcat0701002    Gears of war   \n",
       "2  00033dbced6acd3626c4b56ff5c55b8d69911681  abcat0701002    Gears of war   \n",
       "3  00033dbced6acd3626c4b56ff5c55b8d69911681  abcat0701002  Assassin creed   \n",
       "4  0007756f015345450f7be1df33695421466b7ce4  abcat0701002     dead island   \n",
       "5  000878e35cb70ace315dbdbef54c22477066f07e  abcat0701002       Rocksmith   \n",
       "6  0008b7e06bc3d329b4dc052268e10b98c9121452  abcat0701002         Nba n2k   \n",
       "7  0008f35cccf771838c635196205951403f13da50  abcat0701002    Call of duty   \n",
       "8  000c22a204a9248e3e5458ed52a084b98ddfcf56  abcat0701002       rock band   \n",
       "9  000f573f22d30ead7a0a3d7fce64d3eb1cb8d0be  abcat0701002            xbox   \n",
       "\n",
       "                click_time               query_time  \n",
       "0  2011-10-09 17:22:56.101  2011-10-09 17:21:42.917  \n",
       "1  2011-09-25 13:35:42.198  2011-09-25 13:35:33.234  \n",
       "2  2011-09-25 13:36:08.668  2011-09-25 13:35:33.234  \n",
       "3  2011-09-25 13:37:23.709  2011-09-25 13:37:00.049  \n",
       "4  2011-09-11 15:15:34.336  2011-09-11 15:15:26.206  \n",
       "5  2011-10-25 20:44:36.756  2011-10-25 20:44:22.861  \n",
       "6  2011-10-29 20:10:06.459  2011-10-29 20:09:33.475  \n",
       "7  2011-08-12 18:05:40.887  2011-08-12 18:05:06.414  \n",
       "8  2011-10-07 00:02:54.236  2011-10-07 00:02:44.212  \n",
       "9  2011-09-30 08:39:51.651  2011-09-30 08:36:40.246  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the data to perform modelling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = vectorizer.fit_transform(X['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(query_vec, y, test_size=0.33, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn import svm, metrics, datasets\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] C=1, kernel=linear ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kashi\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... C=1, kernel=linear, score=0.6216834876703777, total= 1.2min\n",
      "[CV] C=1, kernel=linear ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... C=1, kernel=linear, score=0.6263620014810113, total= 1.1min\n",
      "[CV] C=1, kernel=linear ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... C=1, kernel=linear, score=0.634549356223176, total= 1.2min\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] .... C=10, kernel=linear, score=0.6199146810945791, total=  55.1s\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] .... C=10, kernel=linear, score=0.6275256532317782, total= 1.1min\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] .... C=10, kernel=linear, score=0.6305793991416309, total= 1.1min\n",
      "[CV] C=100, kernel=linear ............................................\n",
      "[CV] ... C=100, kernel=linear, score=0.6164811153886172, total= 1.2min\n",
      "[CV] C=100, kernel=linear ............................................\n",
      "[CV] ... C=100, kernel=linear, score=0.6245636305934624, total= 2.7min\n",
      "[CV] C=100, kernel=linear ............................................\n",
      "[CV] ... C=100, kernel=linear, score=0.6282188841201717, total= 2.3min\n",
      "[CV] C=1000, kernel=linear ...........................................\n",
      "[CV] .. C=1000, kernel=linear, score=0.6132556445739257, total= 2.3min\n",
      "[CV] C=1000, kernel=linear ...........................................\n",
      "[CV] .. C=1000, kernel=linear, score=0.6242462710250714, total= 1.6min\n",
      "[CV] C=1000, kernel=linear ...........................................\n",
      "[CV] .. C=1000, kernel=linear, score=0.6259656652360515, total= 1.4min\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.24732077827489335, total= 3.3min\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.2731408018618428, total= 3.7min\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.2709227467811159, total= 3.6min\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.06773488710852149, total= 3.5min\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.06876123981804719, total= 3.8min\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.06974248927038626, total= 3.9min\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.46623660389137445, total= 2.8min\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.4711731725378187, total= 3.4min\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.473068669527897, total= 3.3min\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.24732077827489335, total= 3.5min\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.2731408018618428, total= 4.0min\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.2709227467811159, total= 4.0min\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.5976485277286443, total= 2.7min\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.6056278430128001, total= 2.6min\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.6144849785407726, total= 2.9min\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.46623660389137445, total= 3.0min\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.47159631862900664, total= 3.4min\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.473068669527897, total= 3.0min\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.6230361044636354, total= 2.4min\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.6296413836877182, total= 2.7min\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.634549356223176, total= 2.7min\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.5976485277286443, total= 2.6min\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.6057336295355972, total= 2.9min\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.6144849785407726, total= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed: 252.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']}, {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the best parameters for Grid Searc\n",
    "\n",
    "param_grid = [ {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},]\n",
    "\n",
    "#Create SVM model and fit the training data\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, param_grid, refit = True, verbose = 3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the SKUs for the training dta\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for - \n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']}, {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=None, verbose=3):\n",
      "[[15  0  0 ...  0  0  0]\n",
      " [ 0  4  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0 12  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model using Confustion matrix\n",
    "print(\"Confusion Matrix for - \\n{}:\\n{}\\n\".format(clf, metrics.confusion_matrix(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8887644, 2541184, 9854804, ..., 3552327, 9854804, 2945052],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SKU predictions\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for - \n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']}, {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=None, verbose=3):\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         1004622       0.68      0.83      0.75        18\n",
      "         1010544       1.00      1.00      1.00         4\n",
      "         1011067       0.00      0.00      0.00         1\n",
      "         1011491       1.00      0.35      0.52        23\n",
      "         1011831       0.00      0.00      0.00         3\n",
      "         1012721       0.84      0.89      0.86        53\n",
      "         1012876       1.00      1.00      1.00         2\n",
      "         1013666       0.00      0.00      0.00         7\n",
      "         1014064       1.00      0.67      0.80         3\n",
      "         1032361       0.81      0.70      0.75        67\n",
      "         1052221       0.67      0.25      0.36         8\n",
      "         1066233       1.00      0.22      0.36         9\n",
      "         1066515       0.75      1.00      0.86         6\n",
      "         1066551       0.57      0.82      0.67        44\n",
      "         1067848       0.81      0.72      0.76        18\n",
      "         1078792       0.66      0.93      0.77        42\n",
      "         1092494       0.95      0.98      0.97        64\n",
      "         1121355       1.00      0.14      0.25         7\n",
      "         1142357       0.00      0.00      0.00         2\n",
      "         1154546       0.00      0.00      0.00        12\n",
      "         1161734       0.89      0.50      0.64        16\n",
      "         1170735       0.00      0.00      0.00         2\n",
      "         1179927       0.40      0.36      0.38        11\n",
      "         1179963       0.00      0.00      0.00        10\n",
      "         1180061       0.67      0.33      0.44         6\n",
      "         1180104       0.14      0.30      0.19        61\n",
      "         1182175       0.62      0.24      0.35        54\n",
      "         1199284       0.90      0.56      0.69        16\n",
      "         1207275       0.80      0.22      0.35        18\n",
      "         1208344       0.00      0.00      0.00         0\n",
      "         1208468       0.00      0.00      0.00         1\n",
      "         1208486       0.00      0.00      0.00         1\n",
      "         1208753       1.00      0.50      0.67         2\n",
      "         1228939       0.91      0.72      0.81       198\n",
      "         1228993       0.80      0.73      0.76        11\n",
      "         1251132       0.78      0.95      0.85       112\n",
      "         1309041       1.00      1.00      1.00         2\n",
      "         1315467       1.00      1.00      1.00         2\n",
      "         1315528       0.67      1.00      0.80         2\n",
      "         1318437       0.00      0.00      0.00         1\n",
      "         1324987       0.38      0.75      0.50         4\n",
      "         1331217       0.00      0.00      0.00         5\n",
      "         1338007       0.00      0.00      0.00         7\n",
      "         1338089       1.00      0.11      0.19        19\n",
      "         1404133       0.00      0.00      0.00         3\n",
      "         1404415       1.00      0.75      0.86         4\n",
      "         1431386       0.43      0.74      0.54        31\n",
      "         1440189       0.00      0.00      0.00         2\n",
      "         1443131       0.00      0.00      0.00         1\n",
      "         1450556       0.00      0.00      0.00         1\n",
      "         1470129       0.50      0.25      0.33         4\n",
      "         1475036       0.83      1.00      0.91        10\n",
      "         1493444       0.90      1.00      0.95        19\n",
      "         1493639       0.00      0.00      0.00         1\n",
      "         1508787       1.00      0.29      0.45        17\n",
      "         1511584       0.50      0.05      0.08        22\n",
      "         1515341       0.90      0.81      0.85        54\n",
      "         1535584       0.00      0.00      0.00         2\n",
      "         1535718       1.00      1.00      1.00         1\n",
      "         1563392       0.78      0.37      0.50        19\n",
      "         1563461       0.88      0.93      0.90        15\n",
      "         1579138       0.83      0.71      0.77         7\n",
      "         1580049       0.00      0.00      0.00        57\n",
      "         1617038       0.90      0.24      0.38        37\n",
      "         1685052       0.87      0.87      0.87        23\n",
      "         1775051       0.50      0.55      0.52        11\n",
      "         1776209       1.00      0.88      0.93         8\n",
      "         1778076       0.00      0.00      0.00         2\n",
      "         1802089       1.00      1.00      1.00         1\n",
      "         1807118       0.80      0.94      0.86        17\n",
      "         1808056       0.92      0.81      0.86        27\n",
      "         1814093       0.50      0.67      0.57         3\n",
      "         1816073       0.79      1.00      0.88        26\n",
      "         1930151       0.90      0.96      0.93        27\n",
      "         1953203       0.82      1.00      0.90         9\n",
      "         1972826       0.75      1.00      0.86         3\n",
      "         1973042       0.80      1.00      0.89         4\n",
      "         1974315       0.50      0.09      0.15        22\n",
      "         1981099       1.00      1.00      1.00         2\n",
      "         1981345       0.88      1.00      0.93         7\n",
      "         1989198       0.82      1.00      0.90         9\n",
      "         1991044       1.00      0.15      0.27        13\n",
      "         1997154       1.00      1.00      1.00         2\n",
      "         2010435       0.00      0.00      0.00         1\n",
      "         2032076       0.80      0.08      0.15        98\n",
      "         2051131       1.00      0.89      0.94         9\n",
      "         2075383       1.00      0.67      0.80         3\n",
      "         2078113       0.21      0.37      0.26       434\n",
      "         2095189       0.65      0.86      0.74       120\n",
      "         2106238       1.00      1.00      1.00         3\n",
      "         2107458       0.75      0.89      0.81       861\n",
      "         2138219       0.00      0.00      0.00         3\n",
      "         2138398       0.00      0.00      0.00         1\n",
      "         2141045       0.00      0.00      0.00         0\n",
      "         2146641       0.92      0.92      0.92        13\n",
      "         2173065       0.65      0.93      0.76       696\n",
      "         2173135       0.88      1.00      0.93         7\n",
      "         2202037       0.00      0.00      0.00        31\n",
      "         2212043       0.94      0.75      0.83        20\n",
      "         2248278       0.65      0.84      0.73        31\n",
      "         2281794       1.00      0.67      0.80         3\n",
      "         2284728       1.00      0.50      0.67         4\n",
      "         2319319       1.00      1.00      1.00         8\n",
      "         2330703       0.55      0.81      0.65        58\n",
      "         2340293       0.83      0.99      0.90        69\n",
      "         2375195       0.78      0.84      0.81       321\n",
      "         2396097       1.00      1.00      1.00         2\n",
      "         2432045       0.89      1.00      0.94         8\n",
      "         2432142       1.00      0.82      0.90        11\n",
      "         2432276       0.69      0.79      0.73        42\n",
      "         2467129       0.58      0.08      0.14       329\n",
      "         2467183       0.80      0.14      0.24       251\n",
      "         2480232       1.00      0.91      0.95        11\n",
      "         2524106       1.00      1.00      1.00         3\n",
      "         2525293       1.00      1.00      1.00         1\n",
      "         2540079       0.57      0.80      0.67        10\n",
      "         2541184       0.97      0.95      0.96       820\n",
      "         2542262       1.00      1.00      1.00         2\n",
      "         2542914       1.00      1.00      1.00         8\n",
      "         2548405       1.00      0.50      0.67         2\n",
      "         2565493       0.00      0.00      0.00         1\n",
      "         2576068       0.60      1.00      0.75         9\n",
      "         2598445       0.72      0.65      0.68        20\n",
      "         2598588       0.00      0.00      0.00         1\n",
      "         2613542       0.56      0.84      0.67        82\n",
      "         2628429       0.55      0.86      0.67        97\n",
      "         2633103       0.41      0.85      0.55       226\n",
      "         2633149       0.30      0.02      0.03       173\n",
      "         2633158       0.67      1.00      0.80         6\n",
      "         2643215       0.00      0.00      0.00         6\n",
      "         2663112       0.90      1.00      0.95        26\n",
      "         2670133       0.55      0.51      0.53       784\n",
      "         2671044       0.91      0.84      0.87        49\n",
      "         2677215       1.00      1.00      1.00         2\n",
      "         2678108       1.00      1.00      1.00         3\n",
      "         2678241       0.80      0.27      0.40        15\n",
      "         2678578       0.00      0.00      0.00        14\n",
      "         2703101       1.00      0.60      0.75         5\n",
      "         2704058       0.92      0.91      0.91       293\n",
      "         2712876       0.77      0.91      0.83        11\n",
      "         2722027       0.00      0.00      0.00         1\n",
      "         2730972       0.00      0.00      0.00         2\n",
      "         2731025       1.00      1.00      1.00         1\n",
      "         2737083       0.43      0.83      0.57        18\n",
      "         2737171       0.50      0.14      0.22         7\n",
      "         2749041       0.81      0.73      0.77        52\n",
      "         2755149       0.79      0.87      0.82        38\n",
      "         2758085       0.90      0.90      0.90       117\n",
      "         2758119       1.00      1.00      1.00         3\n",
      "         2759181       1.00      1.00      1.00         1\n",
      "         2760575       0.00      0.00      0.00         3\n",
      "         2769488       0.83      1.00      0.91         5\n",
      "         2774101       0.83      0.83      0.83         6\n",
      "         2795128       0.00      0.00      0.00         3\n",
      "         2802508       0.90      0.88      0.89        95\n",
      "         2807036       0.94      0.98      0.96       128\n",
      "         2808872       0.00      0.00      0.00         2\n",
      "         2809529       1.00      1.00      1.00         7\n",
      "         2815178       0.36      1.00      0.53         4\n",
      "         2823826       0.00      0.00      0.00         1\n",
      "         2823899       0.83      0.14      0.24        36\n",
      "         2824158       1.00      1.00      1.00         4\n",
      "         2833031       0.23      0.10      0.14        79\n",
      "         2833095       0.86      0.92      0.89        13\n",
      "         2842602       0.00      0.00      0.00         2\n",
      "         2842639       0.96      0.65      0.77        37\n",
      "         2842757       0.93      0.87      0.90        15\n",
      "         2842775       0.00      0.00      0.00         1\n",
      "         2846098       0.00      0.00      0.00         2\n",
      "         2856465       1.00      0.12      0.22        24\n",
      "         2856517       0.29      0.62      0.39       107\n",
      "         2856544       0.50      0.09      0.15        67\n",
      "         2889044       0.00      0.00      0.00        21\n",
      "         2897055       0.38      0.44      0.41        81\n",
      "         2897073       0.33      0.08      0.13        25\n",
      "         2897091       0.00      0.00      0.00         2\n",
      "         2897116       0.72      0.73      0.72       102\n",
      "         2903172       1.00      0.96      0.98        53\n",
      "         2935092       0.92      0.71      0.80        17\n",
      "         2938114       1.00      0.60      0.75         5\n",
      "         2945052       0.53      0.71      0.61       601\n",
      "         2953607       0.49      0.88      0.63       357\n",
      "         2967045       0.94      1.00      0.97        16\n",
      "         2977637       0.89      0.79      0.84       325\n",
      "         3001046       0.00      0.00      0.00        39\n",
      "         3021387       0.88      0.77      0.82        30\n",
      "         3035168       0.75      1.00      0.86         3\n",
      "         3046066       0.59      0.14      0.22       139\n",
      "         3046603       0.36      0.02      0.05       324\n",
      "         3052094       0.00      0.00      0.00         1\n",
      "         3052119       0.00      0.00      0.00         2\n",
      "         3055161       0.00      0.00      0.00         7\n",
      "         3071408       0.95      0.95      0.95        41\n",
      "         3071602       0.00      0.00      0.00         7\n",
      "         3088387       0.67      0.56      0.61        18\n",
      "         3130065       0.00      0.00      0.00         1\n",
      "         3152127       0.95      0.95      0.95        22\n",
      "         3166581       1.00      0.12      0.22         8\n",
      "         3180602       0.75      0.43      0.55         7\n",
      "         3184753       0.00      0.00      0.00         1\n",
      "         3184908       0.00      0.00      0.00         4\n",
      "         3209295       0.91      1.00      0.95        10\n",
      "         3244621       0.17      0.02      0.03        55\n",
      "         3301256       1.00      1.00      1.00         2\n",
      "         3301556       1.00      1.00      1.00        21\n",
      "         3302121       1.00      0.50      0.67         2\n",
      "         3432088       0.00      0.00      0.00         3\n",
      "         3433032       0.00      0.00      0.00        13\n",
      "         3447104       0.75      0.87      0.80        45\n",
      "         3447362       1.00      0.80      0.89         5\n",
      "         3447468       1.00      0.50      0.67         2\n",
      "         3455185       0.00      0.00      0.00         4\n",
      "         3455398       0.90      0.97      0.93       122\n",
      "         3487029       0.57      1.00      0.72        13\n",
      "         3508275       0.00      0.00      0.00         1\n",
      "         3519923       0.75      0.27      0.40        11\n",
      "         3552327       0.86      1.00      0.93        51\n",
      "         3555951       0.00      0.00      0.00         4\n",
      "         3558579       1.00      1.00      1.00         2\n",
      "         3559384       0.86      0.60      0.71        20\n",
      "         3559471       0.86      1.00      0.92        12\n",
      "         3559505       0.33      0.08      0.13        12\n",
      "         3559578       0.40      0.88      0.55        26\n",
      "         3566857       1.00      1.00      1.00         3\n",
      "         3569062       0.00      0.00      0.00         1\n",
      "         3569099       0.00      0.00      0.00         1\n",
      "         3569363       0.00      0.00      0.00         1\n",
      "         3581907       0.00      0.00      0.00         1\n",
      "         3581961       0.80      0.57      0.67         7\n",
      "         3600554       0.00      0.00      0.00         2\n",
      "         3617069       0.00      0.00      0.00         1\n",
      "         3617087       0.00      0.00      0.00         4\n",
      "         3619085       0.00      0.00      0.00         0\n",
      "         3622159       1.00      1.00      1.00         4\n",
      "         3648174       0.00      0.00      0.00         1\n",
      "         3650045       1.00      1.00      1.00         1\n",
      "         3650081       1.00      0.50      0.67         6\n",
      "         3650391       1.00      0.67      0.80         3\n",
      "         3650503       0.00      0.00      0.00         3\n",
      "         3651035       0.00      0.00      0.00         1\n",
      "         3655492       0.00      0.00      0.00         1\n",
      "         3668336       0.00      0.00      0.00        10\n",
      "         3686213       0.00      0.00      0.00         2\n",
      "         3687063       0.00      0.00      0.00         2\n",
      "         3720071       1.00      0.50      0.67         2\n",
      "         3864082       0.00      0.00      0.00         1\n",
      "         3967307       0.00      0.00      0.00         1\n",
      "         7294661       1.00      0.02      0.05        43\n",
      "         7849894       1.00      1.00      1.00         1\n",
      "         7959033       0.66      0.95      0.78        43\n",
      "         8025816       0.00      0.00      0.00         1\n",
      "         8115906       0.00      0.00      0.00         1\n",
      "         8237259       0.00      0.00      0.00         3\n",
      "         8237302       0.74      0.97      0.84        36\n",
      "         8323593       0.00      0.00      0.00        15\n",
      "         8438032       0.00      0.00      0.00         4\n",
      "         8461915       0.00      0.00      0.00         3\n",
      "         8504353       1.00      0.50      0.67         2\n",
      "         8533848       0.80      1.00      0.89         4\n",
      "         8534204       1.00      1.00      1.00         6\n",
      "         8564564       0.00      0.00      0.00         6\n",
      "         8590562       0.00      0.00      0.00         2\n",
      "         8595763       0.00      0.00      0.00         8\n",
      "         8597556       0.00      0.00      0.00         1\n",
      "         8669657       0.00      0.00      0.00         0\n",
      "         8709203       0.50      0.50      0.50         2\n",
      "         8736601       0.64      0.82      0.72        11\n",
      "         8760398       0.00      0.00      0.00        29\n",
      "         8770868       0.00      0.00      0.00        14\n",
      "         8793326       1.00      0.25      0.40         4\n",
      "         8813313       0.44      0.80      0.57        15\n",
      "         8814811       0.00      0.00      0.00         8\n",
      "         8829707       0.50      1.00      0.67         1\n",
      "         8878609       0.00      0.00      0.00         3\n",
      "         8887644       0.91      0.77      0.83        13\n",
      "         8947205       0.00      0.00      0.00         3\n",
      "         8949365       1.00      0.25      0.40         4\n",
      "         8963936       0.00      0.00      0.00         1\n",
      "         8988018       1.00      1.00      1.00         1\n",
      "         9018537       1.00      0.25      0.40         4\n",
      "         9064077       0.33      1.00      0.50         1\n",
      "         9092206       1.00      0.62      0.77         8\n",
      "         9092563       1.00      0.62      0.77         8\n",
      "         9122683       0.00      0.00      0.00         7\n",
      "         9166742       0.00      0.00      0.00         2\n",
      "         9166797       0.79      0.88      0.84        26\n",
      "         9189415       0.78      0.88      0.82         8\n",
      "         9194999       1.00      1.00      1.00         1\n",
      "         9215333       1.00      0.79      0.88        14\n",
      "         9215814       0.00      0.00      0.00         1\n",
      "         9223501       1.00      0.29      0.44         7\n",
      "         9238336       0.00      0.00      0.00         1\n",
      "         9254111       1.00      0.10      0.18        41\n",
      "         9254255       1.00      0.83      0.91         6\n",
      "         9261014       0.89      0.73      0.80        11\n",
      "         9261746       1.00      0.67      0.80         3\n",
      "         9265038       0.50      0.25      0.33         4\n",
      "         9265387       1.00      0.50      0.67         2\n",
      "         9276473       0.71      0.10      0.18        48\n",
      "         9285686       1.00      0.25      0.40         4\n",
      "         9291232       1.00      1.00      1.00         4\n",
      "         9328809       0.00      0.00      0.00         6\n",
      "         9328943       0.83      1.00      0.91        15\n",
      "         9331591       0.00      0.00      0.00        10\n",
      "         9331644       1.00      0.69      0.81        16\n",
      "         9333544       0.00      0.00      0.00         1\n",
      "         9345559       0.00      0.00      0.00         3\n",
      "         9374134       0.00      0.00      0.00        14\n",
      "         9385809       1.00      1.00      1.00        11\n",
      "         9406243       1.00      0.70      0.82        10\n",
      "         9417179       1.00      0.67      0.80         3\n",
      "         9441712       0.00      0.00      0.00         2\n",
      "         9460736       0.00      0.00      0.00        41\n",
      "         9461183       0.69      0.92      0.79        24\n",
      "         9487067       0.27      0.19      0.22        21\n",
      "         9504431       1.00      1.00      1.00         1\n",
      "         9511432       0.91      0.91      0.91        23\n",
      "         9515937       0.00      0.00      0.00         1\n",
      "         9522634       0.00      0.00      0.00         1\n",
      "         9540428       0.62      0.71      0.67         7\n",
      "         9562261       1.00      1.00      1.00         1\n",
      "         9562957       0.00      0.00      0.00         0\n",
      "         9589313       0.00      0.00      0.00         1\n",
      "         9589956       0.00      0.00      0.00         2\n",
      "         9622501       0.00      0.00      0.00         2\n",
      "         9629301       1.00      1.00      1.00         3\n",
      "         9629647       1.00      0.50      0.67         2\n",
      "         9649243       1.00      0.78      0.88         9\n",
      "         9663261       0.35      0.94      0.51        18\n",
      "         9691527       0.50      0.88      0.64         8\n",
      "         9701377       0.68      0.34      0.46        38\n",
      "         9707935       1.00      0.22      0.36         9\n",
      "         9713872       0.47      0.42      0.45       111\n",
      "         9715591       0.33      1.00      0.50         1\n",
      "         9739989       0.30      0.75      0.43         4\n",
      "         9790751       0.00      0.00      0.00         1\n",
      "         9791614       0.84      0.53      0.65        30\n",
      "         9791723       0.67      0.17      0.27        12\n",
      "         9808306       0.00      0.00      0.00         0\n",
      "         9810374       0.90      0.82      0.86        22\n",
      "         9823696       0.00      0.00      0.00         0\n",
      "         9823769       0.00      0.00      0.00        17\n",
      "         9825676       0.00      0.00      0.00         1\n",
      "         9827204       0.00      0.00      0.00         9\n",
      "         9843708       0.40      0.67      0.50         3\n",
      "         9854507       0.80      0.12      0.22        32\n",
      "         9854668       0.71      0.24      0.36        62\n",
      "         9854786       0.61      0.85      0.71        20\n",
      "         9854804       0.52      0.79      0.63      1015\n",
      "         9865018       0.95      1.00      0.97        19\n",
      "         9889193       0.62      0.16      0.26        31\n",
      "         9893348       1.00      0.27      0.42        15\n",
      "         9896071       1.00      1.00      1.00         1\n",
      "         9902347       0.33      0.26      0.29       284\n",
      "         9927328       0.78      0.32      0.46        65\n",
      "         9927373       1.00      0.86      0.92         7\n",
      "         9927451       0.00      0.00      0.00         1\n",
      "         9936089       0.78      0.52      0.62        27\n",
      "         9936195       1.00      1.00      1.00         1\n",
      "         9949037       0.33      0.11      0.17         9\n",
      "         9952004       0.00      0.00      0.00         1\n",
      "         9953959       0.00      0.00      0.00         2\n",
      "         9955514       0.55      0.87      0.67        39\n",
      "         9956073       0.00      0.00      0.00         2\n",
      "         9959853       0.83      0.71      0.77         7\n",
      "         9963729       0.00      0.00      0.00         1\n",
      "         9967476       0.60      0.80      0.69        15\n",
      "         9976899       0.92      0.66      0.77        35\n",
      "         9980886       0.00      0.00      0.00         1\n",
      "         9984142       0.63      0.26      0.37        46\n",
      "9999169100050027       0.00      0.00      0.00         1\n",
      "\n",
      "     avg / total       0.64      0.63      0.59     13981\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kashi\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\kashi\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model using Classification report to check Precision, recall and F1-score\n",
    "print(\"Classification Report for - \\n{}:\\n{}\\n\".format(\n",
    "    clf, metrics.classification_report(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the same technique for the entire dataset\n",
    "\n",
    "X_final = train[['query','user']]\n",
    "y_final = train['sku']\n",
    "query_vec1 =  vectorizer.fit_transform(X_final['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('all/test.csv')\n",
    "test[0:10]\n",
    "X_final_test = test[['query','user']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec2 =  vectorizer.fit_transform(X_final_test['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kashi\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] C=1, kernel=linear ..............................................\n",
      "[CV] ..... C=1, kernel=linear, score=0.6303586438778369, total= 5.2min\n",
      "[CV] C=1, kernel=linear ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 11.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... C=1, kernel=linear, score=0.6356424422722765, total= 2.2min\n",
      "[CV] C=1, kernel=linear ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 17.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... C=1, kernel=linear, score=0.6378927778970725, total= 2.4min\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] .... C=10, kernel=linear, score=0.6281171196413561, total= 2.1min\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] .... C=10, kernel=linear, score=0.6344383057090239, total= 2.2min\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] .... C=10, kernel=linear, score=0.6367475484933076, total= 2.2min\n",
      "[CV] C=100, kernel=linear ............................................\n",
      "[CV] .... C=100, kernel=linear, score=0.625245166713365, total= 2.1min\n",
      "[CV] C=100, kernel=linear ............................................\n",
      "[CV] ... C=100, kernel=linear, score=0.6318175378948859, total= 2.3min\n",
      "[CV] C=100, kernel=linear ............................................\n",
      "[CV] ... C=100, kernel=linear, score=0.6340992054971012, total= 2.3min\n",
      "[CV] C=1000, kernel=linear ...........................................\n",
      "[CV] .. C=1000, kernel=linear, score=0.6224432614177641, total= 2.5min\n",
      "[CV] C=1000, kernel=linear ...........................................\n",
      "[CV] .. C=1000, kernel=linear, score=0.6306134013316333, total= 2.6min\n",
      "[CV] C=1000, kernel=linear ...........................................\n",
      "[CV] .. C=1000, kernel=linear, score=0.6328823992556009, total= 2.8min\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.30729896329504064, total= 3.1min\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.31024224394390143, total= 3.4min\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV]  C=1, gamma=0.001, kernel=rbf, score=0.31314866509197625, total= 3.4min\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.06927710843373494, total= 3.3min\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.07005241535628276, total= 3.6min\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV]  C=1, gamma=0.0001, kernel=rbf, score=0.07071791568248514, total= 3.6min\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.4891426169795461, total= 2.7min\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.49575010624734384, total= 2.9min\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV]  C=10, gamma=0.001, kernel=rbf, score=0.4994631737169852, total= 3.0min\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.30729896329504064, total= 3.1min\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.31024224394390143, total= 3.4min\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV]  C=10, gamma=0.0001, kernel=rbf, score=0.31314866509197625, total= 3.4min\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.6107453068086298, total= 2.3min\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.6161637625726023, total= 2.5min\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV]  C=100, gamma=0.001, kernel=rbf, score=0.6213585283802162, total= 2.5min\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.4891426169795461, total= 2.7min\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.49575010624734384, total= 2.9min\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV]  C=100, gamma=0.0001, kernel=rbf, score=0.4994631737169852, total= 2.9min\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.6307789296721771, total= 2.1min\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.6386173678991358, total= 2.3min\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV]  C=1000, gamma=0.001, kernel=rbf, score=0.6386085462744256, total= 2.3min\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.6107453068086298, total= 2.3min\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.6162345941351466, total= 2.5min\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV]  C=1000, gamma=0.0001, kernel=rbf, score=0.6213585283802162, total= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed: 266.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']}, {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the entire training data\n",
    "clf.fit(query_vec1,y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the outcone for Test data\n",
    "final_pred =  clf.predict(query_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9865018, 1161734, 2704058, 2375195, 9739989, 1078792, 2670133,\n",
       "       9854804, 2670133, 9854804, 9276473, 2704058, 2078113, 2945052,\n",
       "       9854804, 2670133, 2107458, 2897116, 2107458, 2945052, 9854804,\n",
       "       2078113, 2173065, 2107458, 1092494, 2712876, 2107458, 2173065,\n",
       "       9691527, 2670133, 2802508, 9854804, 9955514, 2173065, 2670133,\n",
       "       2633103, 1067848, 2807036, 2541184, 2173065, 1563461, 9854804,\n",
       "       2541184, 9902347, 2375195, 2953607, 9854804, 2977637, 9854804,\n",
       "       9854804], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
